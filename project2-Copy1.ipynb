{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch.utils.data as data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jpierre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning set\n",
    "with open('trainDict.pkl', 'rb') as file:\n",
    "    trainDict = pickle.load(file)\n",
    "\n",
    "# small validation set (useless here btw)\n",
    "with open('valDict.pkl', 'rb') as file:\n",
    "    valDict = pickle.load(file)\n",
    "\n",
    "# test set\n",
    "with open('testDict.pkl', 'rb') as file:\n",
    "    testDict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "SOS = '[CLS] '\n",
    "EOS = ' [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing is realized according to the following steps:\n",
    "1) add spaces before punctuation in order to take them as \"words\" later\n",
    "2) get rid of possibly rigged characters\n",
    "3) convert everything to lower cases and get rid of initial and final spaces\n",
    "\n",
    "NB: an option is available in order to get apply everything on each sentence within the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences1:list[str], sentences2:list[str], directTreat:bool = True, addExtremes:bool = True):\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    for i in range(len(sentences1)):\n",
    "\n",
    "        sentence1 = sentences1[i]\n",
    "        sentence2 = sentences2[i]\n",
    "        \n",
    "        if directTreat:\n",
    "            sentence1 = re.sub(r\"([.!?])\", r\" \\1\", sentence1)                             #add a space before '.', '!', '?'\n",
    "            sentence1 = re.sub(r\"[^a-zA-Zà-úÀ-ÚéèëËÉûÛ!?]+\", r\" \", sentence1)               # get rid of bad characters\n",
    "            # new\n",
    "            # sentence1 = re.sub(r\"[^\\w\\s\\']\", r\" \", sentence1)\n",
    "            sentence1 = re.sub(r\" +\", r\" \", sentence1)                                  # case of multiple spaces\n",
    "\n",
    "            sentence2 = re.sub(r\"([.!?])\", r\" \\1\", sentence2)                             #add a space before '.', '!', '?'\n",
    "            sentence2 = re.sub(r\"[^a-zA-Zà-úÀ-ÚéèëËÉûÛ!?]+\", r\" \", sentence2)               # get rid of bad characters\n",
    "            # new\n",
    "            sentence2 = re.sub(r\" +\", r\" \", sentence2)\n",
    "\n",
    "            L1 = len(sentence1.split(' '))\n",
    "            L2 = len(sentence2.split(' '))\n",
    "            if L1 < MAX_LENGTH:\n",
    "                if L2 < MAX_LENGTH:\n",
    "                    if addExtremes:\n",
    "                        res1.append(SOS + sentence1.lower().strip() + EOS)\n",
    "                        res2.append(SOS + sentence2.lower().strip() + EOS)\n",
    "                    else:\n",
    "                        res1.append(sentence1.lower().strip())\n",
    "                        res2.append(sentence2.lower().strip())\n",
    "        else:\n",
    "            sentence1 = sent_tokenize(sentence1)                                          # tackle each sentence individally and perform some steps\n",
    "            sentence2 = sent_tokenize(sentence2)\n",
    "            \n",
    "            if len(sentence1) == len(sentence2):\n",
    "                for i in range(len(sentence1)):\n",
    "                    s1 = sentence1[i]\n",
    "                    s2 = sentence2[i]\n",
    "\n",
    "                    s1 = re.sub(r\"([.!?])\", r\" \\1\", s1)                                       #add a space before '.', '!', '?'\n",
    "                    s1 = re.sub(r\"[^a-zA-Zà-úÀ-ÚéèëËÉûÛ!?]+\", r\" \", s1)                         # get rid of bad characters\n",
    "                    # new\n",
    "                    s1 = re.sub(r\" +\", r\" \", s1)\n",
    "\n",
    "                    s2 = re.sub(r\"([.!?])\", r\" \\1\", s2)                                       #add a space before '.', '!', '?'\n",
    "                    s2 = re.sub(r\"[^a-zA-Zà-úÀ-ÚéèëËÉûÛ!?]+\", r\" \", s2)                         # get rid of bad characters\n",
    "                    # new\n",
    "                    s2 = re.sub(r\" +\", r\" \", s2)\n",
    "\n",
    "\n",
    "                    L1 = len(s1.split(' '))\n",
    "                    L2 = len(s2.split(' '))\n",
    "                    if L1 < MAX_LENGTH:\n",
    "                        if L2 < MAX_LENGTH:\n",
    "                            if addExtremes:\n",
    "                                res1.append(SOS + s1.lower().strip() + EOS)\n",
    "                                res2.append(SOS + s2.lower().strip() + EOS)\n",
    "                            else:\n",
    "                                res1.append(s1.lower().strip())\n",
    "                                res2.append(s2.lower().strip())\n",
    "\n",
    "    return res1, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resEng, resFr = preprocess(trainDict['eng'],trainDict['fr'], directTreat=False, addExtremes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a school on higham hill road was later named edward redhead junior school\n",
      "une école sur higham hill road est nommée plus tard edward redhead junior school\n"
     ]
    }
   ],
   "source": [
    "n = 40\n",
    "print(resEng[n])\n",
    "print(resFr[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the txt file of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(filePatth:str, sentences:list[str])->bool:\n",
    "    with open(filePatth, \"w\", encoding=\"utf-8\") as file:\n",
    "        # Write each sentence followed by a newline character\n",
    "        for sentence in tqdm(sentences):\n",
    "            file.write(sentence + \"\\n\")\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [00:00<00:00, 1644353.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeCSV('engData.txt', resEng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [00:00<00:00, 1259563.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeCSV('frData.txt', resFr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain a str having all the words\n",
    "\n",
    "\n",
    "def getVocAndDicts(data:list[str]):\n",
    "    wordsVoc = []\n",
    "\n",
    "    for text in tqdm(resEng):\n",
    "        wordsVoc.extend(sorted(list(set(text.split(' ')))))\n",
    "        wordsVoc = list(set(wordsVoc))\n",
    "\n",
    "\n",
    "    word2Index = {ch:i for i, ch in enumerate(wordsVoc)}\n",
    "    index2Word = {i:ch for i, ch in enumerate(wordsVoc)}\n",
    "\n",
    "    return wordsVoc, word2Index, index2Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [01:29<00:00, 465.03it/s]\n",
      "100%|██████████| 41474/41474 [01:28<00:00, 466.58it/s]\n"
     ]
    }
   ],
   "source": [
    "wordsVocEng, word2IndexEng, index2WordEng = getVocAndDicts(resEng)\n",
    "wordsVocFr, word2IndexFr, index2WordFr = getVocAndDicts(resFr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mulla', 'meacher', 'geographer', 'marche', 'gromit', 'ethic', 'impacts', 'pebibyte', 'apocalyptic', 'accomplished', 'deputation', 'bruch', 'fuse', 'wissenschaften', 'pa', 'amendment', 'miró', 'wholesalers', 'rejoice', 'heffener', 'karen', 'invader', 'skier', 'courtaulds', 'harris', 'cern', 'crittenden', 'administrate', 'adamstown', 'cherokees', 'ashigaru', 'athletic', 'mercantile', 'bloodlines', 'belsize', 'lgpl', 'chimera', 'alto', 'breakout', 'idries', 'ikenob', 'oboe', 'infrared', 'julie', 'winnsboro', 'stand', 'parsifal', 'athlete', 'utopians', 'revenue', 'biomimicry', 'ahfs', 'endeavors', 'ganga', 'refuted', 'larsen', 'acknowledgement', 'madhyamgram', 'wb', 'finlandssvenska', 'moyo', 'gris', 'accesses', 'stormaktstiden', 'philracom', 'criterion', 'return', 'ketchum', 'hernán', 'zipper', 'imitations', 'charisma', 'stamford', 'keybase', 'nightjet', 'borderlands', 'womersley', 'bower', 'specialties', 'citrate', 'meroitic', 'kangaroo', 'almería', 'hadrian', 'mushroom', 'acreage', 'shahabaz', 'grayson', 'borgo', 'mtn', 'morphed', 'lovely', 'undp', 'aker', 'adults', 'ppe', 'kuhn', 'gorodskoy', 'leyden', 'excerpt']\n"
     ]
    }
   ],
   "source": [
    "print(wordsVocEng[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21116\n",
      "conformist\n"
     ]
    }
   ],
   "source": [
    "print(word2IndexEng['conformist'])\n",
    "print(index2WordEng[21116])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the different embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kinds of embeddings considered:\n",
    "1) Word2Vec\n",
    "2) GloVe\n",
    "3) FastText\n",
    "4) Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "### https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "engCorp = [nltk.word_tokenize(sent) for sent in resEng]\n",
    "frCorp = [nltk.word_tokenize(sent) for sent in resFr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vEng1 = gensim.models.Word2Vec(engCorp, min_count = 1, vector_size=100, window = 5)\n",
    "w2vEng1.save('word2Vec_english_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vEng2 = gensim.models.Word2Vec(engCorp, min_count = 1, vector_size=100, window = 5, sg = 1)\n",
    "w2vEng2.save('word2Vec_english_skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vFr1 = gensim.models.Word2Vec(frCorp, min_count = 1, vector_size=100, window = 5)\n",
    "w2vEng1.save('word2Vec_french_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vFr2 = gensim.models.Word2Vec(frCorp, min_count = 1, vector_size=100, window = 5, sg = 1)\n",
    "w2vFr2.save('word2Vec_french_skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contemporary', 0.9020349383354187),\n",
       " ('visual', 0.8855995535850525),\n",
       " ('literary', 0.8743351101875305),\n",
       " ('renaissance', 0.8739112615585327),\n",
       " ('exhibition', 0.8719915151596069),\n",
       " ('science', 0.869580090045929),\n",
       " ('literature', 0.867002546787262),\n",
       " ('specializing', 0.8665003776550293),\n",
       " ('opera', 0.8648836612701416),\n",
       " ('photography', 0.8637229204177856)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vEng2.wv.most_similar('art')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "### We used glove-py but we could directly train the glove model from the Standford website (https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/jpierre/anaconda3/envs/myenvPy/lib/python3.9/site-packages/glove_pybind.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZN5Glove5trainENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglove\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Corpus, Glove\n",
      "File \u001b[0;32m~/anaconda3/envs/myenvPy/lib/python3.9/site-packages/glove/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglove_pybind\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGlove\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/jpierre/anaconda3/envs/myenvPy/lib/python3.9/site-packages/glove_pybind.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZN5Glove5trainENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "### https://fasttext.cc/docs/en/python-module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'engData.txt'\n",
    "file2 = 'frData.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  11181\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    2182 lr:  0.000000 avg.loss:  2.401508 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "Engmodel1 = fasttext.train_unsupervised(file1, model='skipgram', dim = 100)\n",
    "Engmodel1.save_model(\"Engmodel_skipgram.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  11181\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    3067 lr:  0.000000 avg.loss:  2.595868 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "Engmodel2 = fasttext.train_unsupervised(file1, model=\"cbow\", dim = 100)\n",
    "Engmodel2.save_model(\"Engmodel_cbow.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  11181\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    2250 lr:  0.000000 avg.loss:  2.403181 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "frModel1 = fasttext.train_unsupervised(file1, model='skipgram', dim = 100)\n",
    "frModel1.save_model(\"frModel_skipgram.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  11181\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    3143 lr:  0.000000 avg.loss:  2.599534 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "frModel2 = fasttext.train_unsupervised(file1, model='cbow', dim = 100)\n",
    "frModel2.save_model(\"frModel_cbow.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n",
    "(see https://www.scaler.com/topics/nlp/huggingface-transformers/ or https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpierre/anaconda3/envs/myenvPy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerEng = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerFr = BertTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBertTokens(sentences, tokenizer):\n",
    "    res = []\n",
    "    Lmax = float(\"-inf\")\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.tokenize('[CLS] ' + sentence + ' [SEP]')     # [SEP] and [CLS] already taken into account\n",
    "        res.append(tokens)\n",
    "        if (len(res[-1]) > Lmax):\n",
    "            Lmax = len(res[-1])\n",
    "            \n",
    "    print(Lmax)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [00:15<00:00, 2673.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokensEng = getBertTokens(resEng, tokenizerEng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [00:12<00:00, 3214.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokensFr = getBertTokens(resFr, tokenizerFr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sváfa a donné son nom à helgi et pendant ses batailles elle est toujours là pour lui le protégeant du danger\n",
      "['[CLS]', '[UNK]', 'a', 'donné', 'son', 'nom', 'à', 'hel', '##gi', 'et', 'pendant', 'ses', 'batailles', 'elle', 'est', 'toujours', 'là', 'pour', 'lui', 'le', 'proté', '##geant', 'du', 'danger', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(resFr[0])\n",
    "print(tokensFr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sváfa had given helgi his name and during his battles she was always there for him shielding him from danger\n",
      "['[CLS]', 'sv', '##af', '##a', 'had', 'given', 'he', '##l', '##gi', 'his', 'name', 'and', 'during', 'his', 'battles', 'she', 'was', 'always', 'there', 'for', 'him', 'shielding', 'him', 'from', 'danger', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(resEng[0])\n",
    "print(tokensEng[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider 128 as maximum size\n",
    "Lmax = 128\n",
    "\n",
    "def BertEncode(sentences, tokenizer):\n",
    "    inputIdsList = []\n",
    "    maskList = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        encodedWords = tokenizer.encode_plus(\n",
    "                                    sentence,\n",
    "                                    add_special_tokens = True,\n",
    "                                    max_length = Lmax,\n",
    "                                    padding = 'max_length',\n",
    "                                    return_attention_mask = True,\n",
    "                                    return_tensors = 'pt')\n",
    "        inputIdsList.append(encodedWords['input_ids'])\n",
    "        maskList.append(encodedWords['attention_mask'])\n",
    "    return inputIdsList, maskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [00:22<00:00, 1855.55it/s]\n"
     ]
    }
   ],
   "source": [
    "inputIdsEng, masksEng = BertEncode(resEng, tokenizerEng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41474/41474 [00:20<00:00, 2065.28it/s]\n"
     ]
    }
   ],
   "source": [
    "inputIdsFr, masksFr = BertEncode(resFr, tokenizerFr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'sv', '##af', '##a', 'had', 'given', 'he', '##l', '##gi', 'his', 'name', 'and', 'during', 'his', 'battles', 'she', 'was', 'always', 'there', 'for', 'him', 'shielding', 'him', 'from', 'danger', '[SEP]']\n",
      "26\n",
      "tensor(26)\n"
     ]
    }
   ],
   "source": [
    "# check same size\n",
    "print(tokensEng[0])\n",
    "print(len(tokensEng[0]))\n",
    "print(torch.sum(masksEng[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[UNK]', 'a', 'donné', 'son', 'nom', 'à', 'hel', '##gi', 'et', 'pendant', 'ses', 'batailles', 'elle', 'est', 'toujours', 'là', 'pour', 'lui', 'le', 'proté', '##geant', 'du', 'danger', '[SEP]']\n",
      "25\n",
      "tensor(25)\n"
     ]
    }
   ],
   "source": [
    "print(tokensFr[0])\n",
    "print(len(tokensFr[0]))\n",
    "print(torch.sum(masksFr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "BertEng = BertModel.from_pretrained('bert-base-uncased')\n",
    "BertFr = BertModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "\n",
    "\n",
    "BertEng.eval()\n",
    "BertFr.eval()\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBertEmbedding(sentences, BertModel, tokenizer):\n",
    "    \n",
    "    ## Get the input ids and the masks for all the sentences\n",
    "    \n",
    "    inputIds, masks = BertEncode(sentences, tokenizer)\n",
    "    \n",
    "    ## Compute the Bert embedding for each\n",
    "    \n",
    "    inputIdsTensor = torch.vstack(inputIds)\n",
    "    masksTensor = torch.vstack(masks)\n",
    "    \n",
    "    out = BertModel(inputIdsTensor, attention_mask = masksTensor)[0]\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(masks)):\n",
    "        res.append(out[i, masks[i].squeeze().bool(), :])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "inputIdsTensor = torch.vstack(inputIdsEng[:4])\n",
    "masksTensor = torch.vstack(masksEng[:4])\n",
    "print(BertEng(inputIdsTensor, attention_mask = masksTensor)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "inputIdsTensor = torch.vstack(inputIdsFr[:4])\n",
    "masksTensor = torch.vstack(masksFr[:4])\n",
    "print(BertFr(inputIdsTensor, attention_mask = masksTensor)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 989.22it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1174.71it/s]\n"
     ]
    }
   ],
   "source": [
    "embsEng = getBertEmbedding(resEng[:4], BertModel = BertEng, tokenizer = tokenizerEng)\n",
    "embsFr = getBertEmbedding(resFr[:4], BertModel = BertFr, tokenizer = tokenizerFr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'wesley', 'was', 'promoted', 'to', 'brigadier', 'general', 'in', 'while', 'in', 'kabul', 'afghanistan', '[SEP]']\n",
      "13\n",
      "torch.Size([13, 768])\n"
     ]
    }
   ],
   "source": [
    "print(tokensEng[1])\n",
    "print(len(tokensEng[1]))\n",
    "print(embsEng[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'we', '##sl', '##ey', 'a', 'été', 'promu', 'général', 'de', 'brigade', 'en', 'alors', 'qu', 'il', 'se', 'trouvait', 'à', 'k', '##abo', '##ul', 'en', 'af', '##gh', '##ani', '##stan', '[SEP]']\n",
      "26\n",
      "torch.Size([26, 768])\n"
     ]
    }
   ],
   "source": [
    "print(tokensFr[1])\n",
    "print(len(tokensFr[1]))\n",
    "print(embsFr[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a function to centralize everyhting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(sentences: list[str], model = 'Bert', params:list = []):\n",
    "    if model == 'fasttext':\n",
    "        return model[word]\n",
    "    if model == 'gloVe':\n",
    "        pass\n",
    "    if model == 'word2Vec':\n",
    "        pass\n",
    "    if model == 'Bert':\n",
    "        return getBertEmbedding(sentences, BertModel = params[0], tokenizer = params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textDatasets(Dataset):\n",
    "    \"\"\"\n",
    "    Class to generate data tuples for learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lang1List:list[str], lang2List:list[str]):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "        -----\n",
    "        - `lang1List`: list of sentences to translate\n",
    "        - `lang2List`: list of translated sentences\n",
    "        \"\"\"\n",
    "\n",
    "        self.length = len(lang1List)\n",
    "\n",
    "        # should encode here\n",
    "        \n",
    "        self.lang1List = lang1List\n",
    "        self.lang2List = lang2List\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.lang1List[idx], self.lang2List[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainEng, trainFr = preprocess(trainDict['eng'],trainDict['fr'], directTreat=False, addExtremes=False)\n",
    "valEng, valFr = preprocess(valDict['eng'],valDict['fr'], directTreat=False, addExtremes=False)\n",
    "testEng, testFr = preprocess(testDict['eng'],testDict['fr'], directTreat=False, addExtremes=False)\n",
    "\n",
    "\n",
    "\n",
    "datasetLearning = textDatasets(lang1List=trainEng, lang2List = trainFr)\n",
    "datasetValidation = textDatasets(lang1List=valEng, lang2List = valFr)\n",
    "datasetTest = textDatasets(lang1List=testEng, lang2List = testFr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaderLearning = data.DataLoader(datasetLearning, batch_size = 100, shuffle= True, num_workers = 1)\n",
    "loaderValidation = data.DataLoader(datasetValidation, batch_size = 100, shuffle= True, num_workers = 1)\n",
    "loaderTest = data.DataLoader(datasetTest, batch_size = 100, shuffle= True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the pamacca resort is the northern part of tapanahony and mainly inhabited by the paramaccan people', 'within the reserve spruce typically dominates although some localized areas have major amount of pines', 'national register of historic places registration canal street station post office', 'while bloodshot was self aware to a degree the human machine hybrid was still deemed to inflict unacceptably high levels of collateral damage']\n",
      "\n",
      "['le ressort de pamacca est la partie nord de tapanahony et est principalement habité par le peuple paramacca', 'au sein de la réserve l épicéa domine même si quelques zones circonscrites présentent d importantes quantité de pins a', 'national park service', 'alors que bloodshot était conscient de lui même dans une certaine mesure l hybride homme machine était toujours réputé infliger des niveaux de dommages collatéraux élevés inacceptables']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1762.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1982.42it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(emb1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m emb2 \u001b[38;5;241m=\u001b[39m getEmbedding(s2, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBert\u001b[39m\u001b[38;5;124m'\u001b[39m, params \u001b[38;5;241m=\u001b[39m [BertFr, tokenizerFr])\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43memb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for s1, s2, _ in loaderLearning:\n",
    "    print(list(s1)[0:4])\n",
    "    print(\"\")\n",
    "    print(list(s2)[0:4])\n",
    "    \n",
    "    emb1 = getEmbedding(s1, model = 'Bert', params = [BertEng, tokenizerEng])\n",
    "    print(emb1[0].shape)\n",
    "    emb2 = getEmbedding(s2, model = 'Bert', params = [BertFr, tokenizerFr])\n",
    "    print(emb2[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, inputShape:int, hiddenShape:int, dropout:float=0.2):\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.hiddenShape = hiddenShape\n",
    "        self.dropoutProb = dropout\n",
    "\n",
    "        self.gru = nn.GRU(inputShape, hiddenShape, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        ## [B, L, D]\n",
    "        x = self.dropout(x)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, inputShape:int, outputShape:int, dropout = 0.2):\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "        self.dropoutProb = dropout\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(inputShape, inputShape, batch_first=True)\n",
    "        self.out = nn.Linear(inputShape, outputShape)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbEpoch=40\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjepi1202\u001b[0m (\u001b[33muliege_action_spotting_2022_2023_context\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jpierre/other/NLP/wandb/run-20231202_022946-l5nsumls</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis/runs/l5nsumls' target=\"_blank\">NLP_init</a></strong> to <a href='https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis' target=\"_blank\">https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis/runs/l5nsumls' target=\"_blank\">https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis/runs/l5nsumls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/uliege_action_spotting_2022_2023_context/master_thesis/runs/l5nsumls?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7353f58100>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project = 'master_thesis', name = \"NLP_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerEncoder = torch.optim.Adam(encoder.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "optimizerDecoder = torch.optim.Adam(decoder.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "wandb.watch(model, log = 'all', log_freq=100)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(out, y, criterion):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "for i in range(nbEpoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for sent1, sent2, _ in tqdm(loaderLearning):\n",
    "        \n",
    "        ## get the embeddings of the first sentences (in english)\n",
    "        \n",
    "        sent1 = getEmbedding(sent1, model = model).to(device)\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out =  model(x)\n",
    "        \n",
    "\n",
    "        loss = criterion(out.reshape(-1), y.reshape(-1))\n",
    "        \n",
    "        optimizerEncoder.zero_grad()\n",
    "        optimizerDecoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizerEncoder.step()\n",
    "        optimizerDecoder.step()\n",
    "        \n",
    "        if ((j+1) % 100) == 0:\n",
    "            wandb.log({'epoch': i, 'Training Loss': loss})\n",
    "\n",
    "        if ((j+1) % 5000) == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val, _ in loaderValidation:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    \n",
    "                    out = model(x_val)\n",
    "                    \n",
    "                    val_loss += criterion(out, y_val)\n",
    "                    \n",
    "                wandb.log({'epoch': i, 'Validation Loss': val_loss})\n",
    "            NN.train()\n",
    "\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute bleu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenvPy]",
   "language": "python",
   "name": "conda-env-myenvPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
